{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Sentiment_Single_Inference.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/raynardj/python4ml/blob/master/experiments/Sentiment_Single_Inference.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "wt0dpl1ls1sL",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "cef59fc7-074d-4b6a-8a6d-d08ba8b30d66"
      },
      "cell_type": "code",
      "source": [
        "# For 1st timer, load the google drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "WG2sIvS4tt0C",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "RmWURr5YuDD3",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 564
        },
        "outputId": "8ded2c5a-245a-4bd5-d75d-5469f2494d5d"
      },
      "cell_type": "code",
      "source": [
        "# Install ray's tool box\n",
        "!mkdir -p $HOME/data\n",
        "!!pip install git+https://github.com/raynardj/forge"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Collecting git+https://github.com/raynardj/forge',\n",
              " '  Cloning https://github.com/raynardj/forge to /tmp/pip-req-build-8bcyh_hm',\n",
              " 'Requirement already satisfied (use --upgrade to upgrade): ai-forge==0.1.0 from git+https://github.com/raynardj/forge in /usr/local/lib/python3.6/dist-packages',\n",
              " 'Requirement already satisfied: flask==0.12.4 in /usr/local/lib/python3.6/dist-packages (from ai-forge==0.1.0) (0.12.4)',\n",
              " 'Requirement already satisfied: flask_appbuilder==1.10.0 in /usr/local/lib/python3.6/dist-packages (from ai-forge==0.1.0) (1.10.0)',\n",
              " 'Requirement already satisfied: pandas>=0.18.0 in /usr/local/lib/python3.6/dist-packages (from ai-forge==0.1.0) (0.24.2)',\n",
              " 'Requirement already satisfied: tqdm>=4.25.0 in /usr/local/lib/python3.6/dist-packages (from ai-forge==0.1.0) (4.31.1)',\n",
              " 'Requirement already satisfied: itsdangerous>=0.21 in /usr/local/lib/python3.6/dist-packages (from flask==0.12.4->ai-forge==0.1.0) (1.1.0)',\n",
              " 'Requirement already satisfied: Jinja2>=2.4 in /usr/local/lib/python3.6/dist-packages (from flask==0.12.4->ai-forge==0.1.0) (2.10.1)',\n",
              " 'Requirement already satisfied: Werkzeug>=0.7 in /usr/local/lib/python3.6/dist-packages (from flask==0.12.4->ai-forge==0.1.0) (0.15.2)',\n",
              " 'Requirement already satisfied: click>=2.0 in /usr/local/lib/python3.6/dist-packages (from flask==0.12.4->ai-forge==0.1.0) (7.0)',\n",
              " 'Requirement already satisfied: colorama==0.3.9 in /usr/local/lib/python3.6/dist-packages (from flask_appbuilder==1.10.0->ai-forge==0.1.0) (0.3.9)',\n",
              " 'Requirement already satisfied: Flask-Babel==0.11.1 in /usr/local/lib/python3.6/dist-packages (from flask_appbuilder==1.10.0->ai-forge==0.1.0) (0.11.1)',\n",
              " 'Requirement already satisfied: Flask-Login==0.2.11 in /usr/local/lib/python3.6/dist-packages (from flask_appbuilder==1.10.0->ai-forge==0.1.0) (0.2.11)',\n",
              " 'Requirement already satisfied: Flask-WTF==0.14.2 in /usr/local/lib/python3.6/dist-packages (from flask_appbuilder==1.10.0->ai-forge==0.1.0) (0.14.2)',\n",
              " 'Requirement already satisfied: Flask-OpenID==1.2.5 in /usr/local/lib/python3.6/dist-packages (from flask_appbuilder==1.10.0->ai-forge==0.1.0) (1.2.5)',\n",
              " 'Requirement already satisfied: Flask-SQLAlchemy==2.1 in /usr/local/lib/python3.6/dist-packages (from flask_appbuilder==1.10.0->ai-forge==0.1.0) (2.1)',\n",
              " 'Requirement already satisfied: python-dateutil<3,>=2.3 in /usr/local/lib/python3.6/dist-packages (from flask_appbuilder==1.10.0->ai-forge==0.1.0) (2.8.0)',\n",
              " 'Requirement already satisfied: pytz>=2011k in /usr/local/lib/python3.6/dist-packages (from pandas>=0.18.0->ai-forge==0.1.0) (2019.1)',\n",
              " 'Requirement already satisfied: numpy>=1.12.0 in /usr/local/lib/python3.6/dist-packages (from pandas>=0.18.0->ai-forge==0.1.0) (1.16.2)',\n",
              " 'Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.6/dist-packages (from Jinja2>=2.4->flask==0.12.4->ai-forge==0.1.0) (1.1.1)',\n",
              " 'Requirement already satisfied: Babel>=2.3 in /usr/local/lib/python3.6/dist-packages (from Flask-Babel==0.11.1->flask_appbuilder==1.10.0->ai-forge==0.1.0) (2.6.0)',\n",
              " 'Requirement already satisfied: WTForms in /usr/local/lib/python3.6/dist-packages (from Flask-WTF==0.14.2->flask_appbuilder==1.10.0->ai-forge==0.1.0) (2.2.1)',\n",
              " 'Requirement already satisfied: python3-openid>=2.0 in /usr/local/lib/python3.6/dist-packages (from Flask-OpenID==1.2.5->flask_appbuilder==1.10.0->ai-forge==0.1.0) (3.1.0)',\n",
              " 'Requirement already satisfied: SQLAlchemy>=0.7 in /usr/local/lib/python3.6/dist-packages (from Flask-SQLAlchemy==2.1->flask_appbuilder==1.10.0->ai-forge==0.1.0) (1.3.3)',\n",
              " 'Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.6/dist-packages (from python-dateutil<3,>=2.3->flask_appbuilder==1.10.0->ai-forge==0.1.0) (1.12.0)',\n",
              " 'Requirement already satisfied: defusedxml in /usr/local/lib/python3.6/dist-packages (from python3-openid>=2.0->Flask-OpenID==1.2.5->flask_appbuilder==1.10.0->ai-forge==0.1.0) (0.5.0)',\n",
              " 'Building wheels for collected packages: ai-forge',\n",
              " '  Building wheel for ai-forge (setup.py) ... \\x1b[?25ldone',\n",
              " '\\x1b[?25h  Stored in directory: /tmp/pip-ephem-wheel-cache-ha__i014/wheels/79/da/27/142ebff4ec83438e5bca1075d78c22909c306a7f2478eaaffe',\n",
              " 'Successfully built ai-forge']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "metadata": {
        "id": "CuPg8NHy0dcm",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        },
        "outputId": "7e009914-6650-46cf-da43-a9c792d649da"
      },
      "cell_type": "code",
      "source": [
        "train = pd.read_csv(DATA/'quora_train.csv')\n",
        "\n",
        "split_ = (np.random.rand(len(train))>0.7)\n",
        "train_df = train.loc[~split_].reset_index().drop(\"index\",axis=1)\n",
        "valid_df = train.loc[split_].reset_index().drop(\"index\",axis=1)\n",
        "\n",
        "from forgebox.ftorch.prepro import DF_Dataset,Arr_Dataset,Seq_Dataset, fuse\n",
        "from torch.utils.data.dataloader import DataLoader\n",
        "\n",
        "# Text transformed to sequence data\n",
        "train_txt_ds = Seq_Dataset(\"train\",train_df.question_text,seq_len=SEQ_LEN,vocab_path=DATA/\"vocab.json\",bs= BS ,build_vocab=True,sep_tok = \" \", fixlen=True)\n",
        "valid_txt_ds = Seq_Dataset(\"valid\",valid_df.question_text,seq_len=SEQ_LEN,vocab_path=DATA/\"vocab.json\",bs= BS ,build_vocab=False,sep_tok = \" \", fixlen=True)\n",
        "\n",
        "# Label transformed to 1/0 array\n",
        "train_lbl_ds = Arr_Dataset(train_df.target.values,bs=BS)\n",
        "valid_lbl_ds = Arr_Dataset(valid_df.target.values,bs=BS)\n",
        "\n",
        "# Combine 2 datasets\n",
        "train_ds = fuse(train_txt_ds,train_lbl_ds)\n",
        "valid_ds = fuse(valid_txt_ds,valid_lbl_ds)\n",
        "\n",
        "# Testing data with dataloader\n",
        "dl = DataLoader(train_ds,batch_size=1,shuffle=True)\n",
        "testgen = iter(dl)\n",
        "seq_, label_ = next(testgen)\n",
        "\n",
        "seq_, label_ "
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "train sequence type: <class 'pandas.core.series.Series'>\n",
            "train sequence total_length type: 913544\n",
            "valid sequence type: <class 'pandas.core.series.Series'>\n",
            "valid sequence total_length type: 392578\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([[[  20,   19, 1117,  ...,    0,    0,    0],\n",
              "          [ 154,   16, 1374,  ...,    0,    0,    0],\n",
              "          [  11,   14,   10,  ...,    0,    0,    0],\n",
              "          ...,\n",
              "          [  11,   78,  176,  ...,    0,    0,    0],\n",
              "          [   4,   13,   36,  ...,    0,    0,    0],\n",
              "          [  11,   60,    3,  ...,    0,    0,    0]]]),\n",
              " [tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "           0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "           1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1]])])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "metadata": {
        "id": "GiF84byuuA5W",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from forgebox.ftorch import layers\n",
        "import forge"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "_piiHWmLuoyw",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "deec60e4-60e0-47c0-b445-5afe3d943447"
      },
      "cell_type": "code",
      "source": [
        "from pathlib import Path\n",
        "DATA = Path('/content/gdrive/My Drive/data/quora/')\n",
        "\n",
        "import torch\n",
        "DIM = 300\n",
        "CUDA = torch.cuda.is_available()\n",
        "BS = 64\n",
        "MODEL_NAME = \"lstm\"\n",
        "TRY_NAME = \"smaller_hidden\"\n",
        "SEQ_LEN = 30\n",
        "\n",
        "print(\"Is gpu available\",CUDA)"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Is gpu available False\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "psEWFWYSzbR-",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from forgebox.ftorch.layers import LayerNorm,GELU,Attention, MultiHeadedAttention,TransformerBlock\n",
        "import torch\n",
        "from torch import nn\n",
        "\n",
        "class LM_RNN_ATTN(nn.Module):\n",
        "    def __init__(self,vocab_size,\n",
        "                 hs = DIM):\n",
        "        super().__init__()\n",
        "\n",
        "        self.vocab_size = vocab_size\n",
        "        self.emb = nn.Embedding(self.vocab_size,hs)\n",
        "        self.attention = MultiHeadedAttention(4,hs)\n",
        "        self.rnn = nn.LSTM(input_size = hs, hidden_size = hs,num_layers = 1,batch_first = True)\n",
        "        \n",
        "        self.output = nn.Sequential(*[\n",
        "            nn.Linear(hs*2,512), \n",
        "            nn.ReLU(),\n",
        "            nn.Linear(512,1),\n",
        "            nn.Sigmoid(),\n",
        "                                     ])\n",
        "        \n",
        "    def forward(self,x):\n",
        "        x = self.emb(x)\n",
        "        x1 = self.attention(x,x,self.rnn(x)[0])\n",
        "        x2 = self.attention(x,x,self.rnn(x.flip(1))[0])\n",
        "        x = torch.cat([x1[:,-1,:],x2[:,-1,:]], dim = -1)\n",
        "\n",
        "        x = self.output(x)\n",
        "        return x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "7Y5AEpnN0BP_",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "md = LM_RNN_ATTN(train_txt_ds.vocab_size, DIM)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "nwOvEP9o3eCb",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from forgebox.ftorch.utils import load_model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "YCLM_sui3pyb",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "load_model(md,DATA/\"attn_rnn_cpu.npy\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "hDPxQ_GL3zQi",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}